{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ua-datalab/geospatial_2025/blob/main/notebooks/deepforest_colab_e.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d05c59d-33da-44e3-a211-8efef2062846",
      "metadata": {
        "id": "3d05c59d-33da-44e3-a211-8efef2062846"
      },
      "source": [
        "## Tree Detection with DeepForest\n",
        "\n",
        "This jupyter notebook uses the python library DeepForest to identify and put bounding boxes around trees.\n",
        "\n",
        "If using the software, please cite as:\n",
        "Geographic Generalization in Airborne RGB Deep Learning Tree Detection Ben. G. Weinstein, Sergio Marconi, Stephanie A. Bohlman, Alina Zare, Ethan P. White bioRxiv 790071; doi: https://doi.org/10.1101/790071\n",
        "\n",
        "Documentation for DeepForest can be found at https://deepforest.readthedocs.io/en/latest/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69370d7e-bcea-499b-b3fd-605389f1d5a1",
      "metadata": {
        "id": "69370d7e-bcea-499b-b3fd-605389f1d5a1"
      },
      "outputs": [],
      "source": [
        "#Install the deepforest python library. After installing, you may need to restart the kernel before moving to the next code snippet\n",
        "!pip install DeepForest --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall the current version of albumentations\n",
        "!pip uninstall -y albumentations\n",
        "\n",
        "# Install a compatible version of albumentations\n",
        "!pip install albumentations==1.4.24"
      ],
      "metadata": {
        "id": "GmLVR3EyS-w-"
      },
      "id": "GmLVR3EyS-w-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a0e049-5661-4b76-a0c6-21377d9110be",
      "metadata": {
        "id": "c2a0e049-5661-4b76-a0c6-21377d9110be"
      },
      "outputs": [],
      "source": [
        "##After restarting the kernel, import libraries into environment...\n",
        "from deepforest import main\n",
        "from deepforest import get_data\n",
        "from deepforest.utilities import boxes_to_shapefile\n",
        "from deepforest.utilities import shapefile_to_annotations\n",
        "from deepforest.preprocess import split_raster\n",
        "from deepforest.visualize import plot_predictions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import numpy\n",
        "import rasterio\n",
        "import geopandas as gpd\n",
        "from rasterio.plot import show\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6742c0c-5c29-46eb-bd60-534ddecb307a",
      "metadata": {
        "id": "b6742c0c-5c29-46eb-bd60-534ddecb307a"
      },
      "outputs": [],
      "source": [
        "#Bring a DeepForest pretrained model into environment. It is trained to identify trees from aerial imagery\n",
        "#It is located at https://huggingface.co/weecology/deepforest-tree\n",
        "model = main.deepforest()\n",
        "model.load_model(model_name=\"weecology/deepforest-tree\", revision=\"main\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's use GPU for prediction and training\n",
        "print(\"Current device is {}\".format(model.device))\n",
        "model.to(\"cuda\")\n",
        "print(\"Current device is {}\".format(model.device))"
      ],
      "metadata": {
        "id": "ANONIkd3afJz"
      },
      "id": "ANONIkd3afJz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "27819458-7055-4330-ab31-a8e6be7e25d6",
      "metadata": {
        "id": "27819458-7055-4330-ab31-a8e6be7e25d6"
      },
      "source": [
        "## Predict Tree Crowns on Raw (non-georeferenced images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Path for the image you want to ID trees.\n",
        "#These are non-georeferenced single jpeg drone image located in Cyverse datastore\n",
        "# 720 x 540 pixels\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/commons/cyverse_curated/Gillan_Ecosphere_2021/raw_images/May_2019/15-g2/100_0123/100_0123_0086.JPG\n",
        "image_path = get_data(\"/content/100_0123_0086.JPG\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/DJI_0184.jpeg\n",
        "image_path2 = get_data(\"/content/DJI_0184.jpeg\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/100_0407_0064.jpeg\n",
        "image_path3 = get_data(\"/content/100_0407_0064.jpeg\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/DJI_0468.jpeg\n",
        "image_path4 = get_data(\"/content/DJI_0468.jpeg\")\n",
        "\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/101_0472_0074.jpeg\n",
        "image_path5 = get_data(\"/content/101_0472_0074.jpeg\")\n"
      ],
      "metadata": {
        "id": "YK8NqKlO_qm_"
      },
      "id": "YK8NqKlO_qm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5e2b9f-0501-4bff-bd9e-64eeb0ef7c3e",
      "metadata": {
        "id": "cd5e2b9f-0501-4bff-bd9e-64eeb0ef7c3e"
      },
      "outputs": [],
      "source": [
        "#Identify and put bounding boxes around all trees in the image\n",
        "#This will create a table showing image coordinates of every predicted tree\n",
        "#The 'score' is the confidence that the prediction is correct. Values closer to 1 are better.\n",
        "trees = model.predict_image(path=image_path2, return_plot = False)\n",
        "trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99105ef-36ed-420e-aed4-0b83eb7608c9",
      "metadata": {
        "id": "e99105ef-36ed-420e-aed4-0b83eb7608c9"
      },
      "outputs": [],
      "source": [
        "#Show the image with the bounding boxes\n",
        "plot = model.predict_image(path=image_path2, return_plot = True, color=(0, 255, 255), thickness=6)\n",
        "plt.imshow(plot[:,:,::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c01de3a-c409-48db-aa3e-9d42239e327f",
      "metadata": {
        "id": "5c01de3a-c409-48db-aa3e-9d42239e327f"
      },
      "source": [
        "## Predict Tree Crowns on Georeferenced Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff50dfc-dd00-43ef-8bc8-c5834427e5fc",
      "metadata": {
        "id": "4ff50dfc-dd00-43ef-8bc8-c5834427e5fc"
      },
      "outputs": [],
      "source": [
        "#Set the path for a georeferenced image you want to predict tree crowns\n",
        "#This example image is 735 mb and 10088 x 26175 pixels\n",
        "!wget https://data.cyverse.org/dav-anon/iplant/projects/cyverse_training/datalab/nextgen_geospatial/hole_17_ortho_utm.tif\n",
        "raster_path = get_data(\"/content/hole_17_ortho_utm.tif\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b6a04e-4ca0-4d51-9982-aa47f93e11d0",
      "metadata": {
        "id": "c5b6a04e-4ca0-4d51-9982-aa47f93e11d0"
      },
      "outputs": [],
      "source": [
        "##Predict tree crowns on a georeferenced image\n",
        "predicted_raster = model.predict_tile(raster_path, return_plot = True, patch_size=600, patch_overlap=0.25, color=(255, 255, 0), thickness=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a53f0a-ca24-4e82-875f-1a69b7c869e7",
      "metadata": {
        "id": "d8a53f0a-ca24-4e82-875f-1a69b7c869e7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(predicted_raster)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce5727a-1bba-4f45-b982-2fbd82285ff8",
      "metadata": {
        "id": "7ce5727a-1bba-4f45-b982-2fbd82285ff8"
      },
      "source": [
        "## Improve Model with Training\n",
        "If the pre-trained model does not identify all trees correclty, then we want to improve the model by adding some training data and fine-tuning the model.\n",
        "Manual labeling of trees (bounding boxes) can be done in Label Studio."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split raster into 1200x1200 pixel chips\n",
        "\n",
        "from deepforest import preprocess\n",
        "\n",
        "#large geospatial image, probably a geotiff\n",
        "train_image_path = get_data(\"/content/hole_17_ortho_utm.tif\")\n",
        "\n",
        "#output directory on colab\n",
        "output_dir = \"/content/chip\"\n",
        "\n",
        "#parameters for splitting\n",
        "output_crops = preprocess.split_raster(\n",
        "    path_to_raster=train_image_path,\n",
        "    annotations_file=None,   # no labels yet\n",
        "    save_dir=output_dir,\n",
        "    patch_size=1200,          # chip size (pixels)\n",
        "    patch_overlap=0        # overlap to capture cut-off trees\n",
        ")\n",
        "\n",
        "print(f\"Created {len(output_crops)} chips in {output_dir}\")"
      ],
      "metadata": {
        "id": "XllqPSWwzs1q"
      },
      "id": "XllqPSWwzs1q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f8fdf6c"
      },
      "source": [
        "##Download the chips to your local machine\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Replace 'your_directory_name' with the name of the directory you want to download\n",
        "directory_to_download = '/content/chip/'\n",
        "zip_filename = f'{directory_to_download}.zip'\n",
        "\n",
        "# Zip the directory\n",
        "!zip -r \"$zip_filename\" \"$directory_to_download\"\n",
        "\n",
        "# Download the zipped file\n",
        "files.download(zip_filename)\n",
        "\n",
        "# Optional: Remove the zipped file after downloading\n",
        "# os.remove(zip_filename)"
      ],
      "id": "7f8fdf6c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bring your images chips into Label Studio on your local machine, makes labels of trees, export the labels (annotatations & images) in pascal VOC xml format. Make sure the label is 'Tree' not 'tree'."
      ],
      "metadata": {
        "id": "2Hd4EIkh1Jxr"
      },
      "id": "2Hd4EIkh1Jxr"
    },
    {
      "cell_type": "code",
      "source": [
        "#Bring annotations and chips (single zip file) into Colab\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile, io, os\n",
        "\n",
        "# Upload your zip\n",
        "uploaded = files.upload()\n",
        "zip_path = next(iter(uploaded.keys()))\n",
        "\n",
        "# Unzip\n",
        "base_dir = \"/content/labels\"\n",
        "with zipfile.ZipFile(io.BytesIO(uploaded[zip_path])) as z:\n",
        "    z.extractall(base_dir)\n",
        "\n",
        "# Inspect to see folder names\n",
        "print(\"Contents of dataset:\", os.listdir(base_dir))"
      ],
      "metadata": {
        "id": "QQ7CfzYYYyZK"
      },
      "id": "QQ7CfzYYYyZK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert pascal voc xml annotation format to csv format that deepforest wants\n",
        "\n",
        "from deepforest.utilities import read_pascal_voc\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "# Directory with your VOC XMLs\n",
        "VOC_DIR = Path(\"/content/labels/Annotations\")\n",
        "\n",
        "# Collect all XML files into one DataFrame\n",
        "dfs = []\n",
        "for xml in glob.glob(str(VOC_DIR / \"*.xml\")):\n",
        "    df = read_pascal_voc(xml)\n",
        "    dfs.append(df)\n",
        "\n",
        "all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Save as CSV for training\n",
        "all_df.to_csv(\"/content/deepforest_annotations.csv\", index=False)\n",
        "print(all_df.head())\n"
      ],
      "metadata": {
        "id": "o_B_3oGTbcUO"
      },
      "id": "o_B_3oGTbcUO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "38c55fd6-6309-4e27-b5b3-6c296a862990",
      "metadata": {
        "id": "38c55fd6-6309-4e27-b5b3-6c296a862990"
      },
      "source": [
        "### Split annotation data into training and validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Path to your consolidated annotations CSV\n",
        "CSV_IN = \"/content/deepforest_annotations.csv\"\n",
        "TRAIN_OUT = \"/content/deepforest_train.csv\"\n",
        "VAL_OUT   = \"/content/deepforest_valid.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_IN)\n",
        "\n",
        "# Unique images/chips\n",
        "images = df[\"image_path\"].dropna().unique()\n",
        "\n",
        "# 75/25 split at the image level (reproducible)\n",
        "train_imgs, val_imgs = train_test_split(\n",
        "    images, test_size=0.25, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Filter rows by image split\n",
        "train_df = df[df.image_path.isin(train_imgs)].copy()\n",
        "val_df   = df[df.image_path.isin(val_imgs)].copy()\n",
        "\n",
        "# Save\n",
        "train_df.to_csv(TRAIN_OUT, index=False)\n",
        "val_df.to_csv(VAL_OUT, index=False)\n",
        "\n",
        "# Sanity checks & summary\n",
        "print(\"Images — total/train/val:\", len(images), len(train_imgs), len(val_imgs))\n",
        "print(\"Boxes  — total/train/val:\", len(df), len(train_df), len(val_df))\n",
        "\n",
        "# Leakage check\n",
        "leak = set(train_imgs).intersection(set(val_imgs))\n",
        "print(\"Leakage (should be 0):\", len(leak))\n"
      ],
      "metadata": {
        "id": "oL8TFeaI4UHn"
      },
      "id": "oL8TFeaI4UHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "61c8ac15-e8ed-4ffa-97c0-5ec1963cfae1",
      "metadata": {
        "id": "61c8ac15-e8ed-4ffa-97c0-5ec1963cfae1"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c93d670-efc9-4a47-bca8-7157425ba6b2",
      "metadata": {
        "id": "4c93d670-efc9-4a47-bca8-7157425ba6b2"
      },
      "outputs": [],
      "source": [
        "##Set parameters for the training run\n",
        "\n",
        "#Define the pre-trained model (done earlier in the notebook)\n",
        "#model = main.deepforest()\n",
        "\n",
        "model.config['gpus'] = '-1' #move to GPU and use all the GPU resources\n",
        "\n",
        "#model.config[\"save-snapshot\"] = False\n",
        "model.config[\"train\"][\"fast_dev_run\"] = False\n",
        "\n",
        "#The annotation table\n",
        "model.config[\"train\"][\"csv_file\"] = TRAIN_OUT\n",
        "#The directory where the training imagery is located\n",
        "model.config[\"train\"][\"root_dir\"] = \"/content/labels/images\"\n",
        "\n",
        "model.config[\"validation\"][\"csv_file\"] = VAL_OUT\n",
        "model.config[\"validation\"][\"root_dir\"] = \"/content/labels/images\"\n",
        "\n",
        "model.config[\"train\"][\"epochs\"] = 5          # 4 will run, but 8–20 is more typical\n",
        "model.config[\"train\"][\"batch_size\"] = 2       # if out of memory, drop to 1; if comfy, try 4\n",
        "model.config[\"workers\"] = 2                   # dataloader threads; bump if IO-bound\n",
        "model.config[\"score_thresh\"] = 0.4\n",
        "\n",
        "\n",
        "model.create_trainer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5f8efb-a7e2-4c75-acc4-ea5ebcb0dc50",
      "metadata": {
        "id": "6d5f8efb-a7e2-4c75-acc4-ea5ebcb0dc50"
      },
      "outputs": [],
      "source": [
        "##TRAIN THE MODEL!\n",
        "start_time = time.time()\n",
        "model.trainer.fit(model)\n",
        "print(f\"--- Training on GPU: {(time.time() - start_time):.2f} seconds ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d425f1a4-b4b7-4887-8c78-f97a0ff42794",
      "metadata": {
        "id": "d425f1a4-b4b7-4887-8c78-f97a0ff42794"
      },
      "source": [
        "## Visualize the prediction after model fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd1b40d-5974-4c53-aa68-97471ad6f918",
      "metadata": {
        "id": "6cd1b40d-5974-4c53-aa68-97471ad6f918"
      },
      "outputs": [],
      "source": [
        "##Predict tree crowns on a georeferenced image\n",
        "predicted_raster = model.predict_tile(raster_path, return_plot = True, patch_size=1000, patch_overlap=0.25, color=(255, 255, 0), thickness=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f1f1f4-9550-4e01-a775-8f90cbbe39bd",
      "metadata": {
        "id": "75f1f1f4-9550-4e01-a775-8f90cbbe39bd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(predicted_raster)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa637b2-220c-4c17-ac70-92710974a06c",
      "metadata": {
        "id": "8fa637b2-220c-4c17-ac70-92710974a06c"
      },
      "source": [
        "## Output and save prediction results for each image crop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Folder where you want results to go\n",
        "save_dir = \"/content/pred_result\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Evaluate the training set\n",
        "results = model.evaluate(\n",
        "    TRAIN_OUT,      # CSV of training annotations\n",
        "    IMG_ROOT,       # directory containing the chip images\n",
        "    iou_threshold=0.4,\n",
        "    savedir=save_dir\n",
        ")"
      ],
      "metadata": {
        "id": "k57s1Y5FfY3S"
      },
      "id": "k57s1Y5FfY3S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "22989035-5aaa-468f-836f-47c8dd3d4497",
      "metadata": {
        "id": "22989035-5aaa-468f-836f-47c8dd3d4497"
      },
      "source": [
        "## Assessing the Quality of our Tree Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5933db49-0923-419b-8384-d7fca01bf2e0",
      "metadata": {
        "id": "5933db49-0923-419b-8384-d7fca01bf2e0"
      },
      "outputs": [],
      "source": [
        "#show assessment of results\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd50637a-023a-4d44-8f23-59a11dba49d6",
      "metadata": {
        "id": "dd50637a-023a-4d44-8f23-59a11dba49d6"
      },
      "outputs": [],
      "source": [
        "results['results']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f40aa4b-5c30-4222-a72f-6e64cca2950d",
      "metadata": {
        "id": "9f40aa4b-5c30-4222-a72f-6e64cca2950d"
      },
      "outputs": [],
      "source": [
        "results['box_precision']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e60052e-ea87-4881-9f05-a58602742d03",
      "metadata": {
        "id": "3e60052e-ea87-4881-9f05-a58602742d03"
      },
      "outputs": [],
      "source": [
        "results[\"box_recall\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b9ffcf2-956e-40fd-8b7a-ff865941ee0b",
      "metadata": {
        "id": "3b9ffcf2-956e-40fd-8b7a-ff865941ee0b"
      },
      "outputs": [],
      "source": [
        "results[\"class_recall\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef89f449-f3e9-4b62-a244-bd2e337b9693",
      "metadata": {
        "id": "ef89f449-f3e9-4b62-a244-bd2e337b9693"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "328a1c0e-f62d-4403-9183-55eef4f1d410",
      "metadata": {
        "id": "328a1c0e-f62d-4403-9183-55eef4f1d410"
      },
      "source": [
        "## Save and load the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acd3841-da3c-471f-95a1-03ea763d6724",
      "metadata": {
        "id": "3acd3841-da3c-471f-95a1-03ea763d6724"
      },
      "outputs": [],
      "source": [
        "#Save the fine-tuned model out to your storage\n",
        "save_model_dir = os.path.join(savedir, 'golf_course_deepforest.pt')\n",
        "torch.save(model.model.state_dict(),save_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c700fd-4958-42c1-aa83-b79c26039c24",
      "metadata": {
        "id": "87c700fd-4958-42c1-aa83-b79c26039c24"
      },
      "outputs": [],
      "source": [
        "#Bring existing model into environment\n",
        "fine_tuned_model = main.deepforest()\n",
        "fine_tuned_model.model.load_state_dict(torch.load(save_model_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Fine-tuned model to Hugging Face"
      ],
      "metadata": {
        "id": "BwbYKPW1r9y2"
      },
      "id": "BwbYKPW1r9y2"
    },
    {
      "cell_type": "code",
      "source": [
        "#Install python libraries that allow you to connect with Hugging Face\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "2R9bSuBusGn-"
      },
      "id": "2R9bSuBusGn-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input your Hugging Face username toke to authenticate your account\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "bcvvHTv0sGdH"
      },
      "id": "bcvvHTv0sGdH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Push fine-tuned model up to Hugging Face\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Set up repository details\n",
        "repo_name = \"deepforest_fine_tuning\"\n",
        "model_file = \"/content/golf_course_deepforest.pt\"\n",
        "\n",
        "# Create a new repo if it doesn't exist\n",
        "#api = HfApi()\n",
        "#api.create_repo(repo_name)\n",
        "\n",
        "# Upload model to Hugging Face\n",
        "api.upload_file(\n",
        "    path_or_fileobj=model_file,   # Path to your .pt file\n",
        "    repo_id=f\"jgillan/{repo_name}\",\n",
        "    path_in_repo=\"golf_course_deepforest.pt\"  # The name you want for the file on the Hub\n",
        ")\n"
      ],
      "metadata": {
        "id": "TtiYci6VuOQh"
      },
      "id": "TtiYci6VuOQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a model from Hugging Face and bring into Colab"
      ],
      "metadata": {
        "id": "A-HtLOST9Qju"
      },
      "id": "A-HtLOST9Qju"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Download the .pt file from Hugging Face\n",
        "model_file = hf_hub_download(repo_id=\"jgillan/deepforest_fine_tuning\", filename=\"golf_course_deepforest.pt\")\n",
        "\n",
        "fine_tuned_model = main.deepforest()\n",
        "fine_tuned_model.model.load_state_dict(torch.load(model_file))"
      ],
      "metadata": {
        "id": "fne1cd-x5cH9"
      },
      "id": "fne1cd-x5cH9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}