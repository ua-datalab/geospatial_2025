# DeepForest Hyperparameter Optimization Report

**Date:** September 30, 2025
**Total Experiments:** 72
**Success Rate:** 100% (72/72 completed)
**IoU Threshold:** 0.2

---

## Terminology & Definitions

Understanding the key terms used throughout this report:

### Hyperparameters (Training Configuration)

**Batch Size**
- Number of training images processed together in one forward/backward pass through the neural network
- Smaller batches (8): More frequent weight updates, better gradient estimates, higher memory efficiency
- Larger batches (32): Faster training on GPU, more stable gradients, but may miss fine-grained patterns
- *Example:* Batch size 8 means the model looks at 8 image patches, calculates error, then updates its weights

**Epochs**
- One complete pass through the entire training dataset
- More epochs = more learning opportunities, but risk of overfitting (memorizing training data)
- *Example:* 15 epochs means the model sees every training image 15 times during learning

**Workers**
- Number of parallel CPU processes loading and preprocessing training data
- More workers = faster data loading, less GPU idle time
- Optimal number depends on CPU cores and I/O speed
- *Example:* 4 workers means 4 CPU threads simultaneously prepare images while GPU trains

**Patch Size**
- Dimensions (in pixels) of image chips fed to the model during training
- Larger patches: More spatial context, but require more memory
- Smaller patches: Better detail capture, more training samples from same image
- *Example:* Patch size 800 means each training image is 800Ã—800 pixels

### Evaluation Metrics (Model Performance)

**IoU (Intersection over Union)**
- Measures overlap between predicted tree bounding box and ground truth box
- Formula: (Area of Overlap) / (Area of Union)
- Range: 0.0 (no overlap) to 1.0 (perfect match)
- **IoU Threshold 0.2** means predictions with â‰¥20% overlap count as correct detections
- *Example:* If predicted box covers 30% of true tree location, IoU might be 0.25 (above 0.2 threshold = correct)

**Box Precision**
- Proportion of predicted tree boxes that are actually correct (true positives)
- Formula: True Positives / (True Positives + False Positives)
- Range: 0.0 to 1.0 (higher is better)
- Answers: "Of all the trees the model detected, what percentage were real trees?"
- *Example:* Precision 0.8123 = 81.23% of detected trees are genuine (18.77% are false alarms)

**Box Recall**
- Proportion of actual trees (ground truth) that the model successfully detected
- Formula: True Positives / (True Positives + False Negatives)
- Range: 0.0 to 1.0 (higher is better)
- Answers: "Of all the real trees in the image, what percentage did the model find?"
- *Example:* Recall 1.0 = 100% of real trees were detected (zero trees missed)

**F1 Score**
- Harmonic mean of precision and recall, balancing both metrics
- Formula: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
- Range: 0.0 to 1.0 (higher is better)
- Useful when you care equally about false positives and false negatives
- *Example:* F1 = 0.8964 with precision=0.8123 and recall=1.0 means excellent overall detection performance

### Understanding the Precision-Recall Tradeoff

- **High Precision, Low Recall:** Model is conservativeâ€”only detects trees it's very confident about (misses some real trees)
- **Low Precision, High Recall:** Model is aggressiveâ€”detects most real trees but also flags many false positives
- **Perfect Recall (1.0):** Found every single real tree, but might have some false detections
- **Balanced (High F1):** Good at both finding real trees AND avoiding false alarms

---

## Executive Summary

A comprehensive grid search was conducted across 72 hyperparameter combinations to optimize DeepForest tree detection performance. The search space included:

- **Batch Size:** [8, 16, 32] - Number of images per training step
- **Epochs:** [15, 20, 25, 30] - Complete passes through training data
- **Workers:** [2, 4] - Parallel data loading processes
- **Patch Size:** [800, 1000, 1200] - Image dimensions in pixels

---

## Best Model Configuration

**ðŸ† Experiment #4 achieved the highest F1 score of 0.8964**

| Parameter | Value |
|-----------|-------|
| Batch Size | 8 |
| Epochs | 15 |
| Workers | 4 |
| Patch Size | 800 |
| **Box Precision** | **0.8123** |
| **Box Recall** | **1.0000** |
| **F1 Score** | **0.8964** |
| Training Time | 37.23s |
| Model Path | `full_optimization_results/best_model_exp_4.pt` |

---

## Top 10 Configurations

| Rank | Exp ID | F1 Score | Precision | Recall | Batch | Epochs | Workers | Patch |
|------|--------|----------|-----------|--------|-------|--------|---------|-------|
| 1 | 4 | 0.8964 | 0.8123 | 1.0000 | 8 | 15 | 4 | 800 |
| 2 | 35 | 0.8691 | 0.7685 | 1.0000 | 16 | 20 | 4 | 1000 |
| 3 | 66 | 0.8610 | 0.7560 | 1.0000 | 32 | 25 | 4 | 1200 |
| 4 | 62 | 0.8521 | 0.7810 | 0.9375 | 32 | 25 | 2 | 1000 |
| 5 | 24 | 0.8408 | 0.7254 | 1.0000 | 8 | 30 | 4 | 1200 |
| 6 | 37 | 0.8386 | 0.7220 | 1.0000 | 16 | 25 | 2 | 800 |
| 7 | 19 | 0.8333 | 0.7500 | 0.9375 | 8 | 30 | 2 | 800 |
| 8 | 61 | 0.8310 | 0.7462 | 0.9375 | 32 | 25 | 2 | 800 |
| 9 | 45 | 0.8303 | 0.7542 | 0.9236 | 16 | 30 | 2 | 1200 |
| 10 | 12 | 0.8270 | 0.7487 | 0.9236 | 8 | 20 | 4 | 1200 |

---

## Parameter Analysis

### Batch Size Impact

| Batch Size | Avg F1 | Max F1 | Min F1 | Std Dev |
|------------|--------|--------|--------|---------|
| 8 | 0.7795 | 0.8964 | 0.6933 | 0.0548 |
| 16 | 0.7777 | 0.8691 | 0.6957 | 0.0473 |
| 32 | 0.7619 | 0.8610 | 0.6524 | 0.0540 |

**Finding:** Batch size 8 achieved the highest maximum F1 (0.8964) and best average performance (0.7795).

### Epochs Impact

| Epochs | Avg F1 | Max F1 | Min F1 | Std Dev |
|--------|--------|--------|--------|---------|
| 15 | 0.7505 | 0.8964 | 0.6524 | 0.0625 |
| 20 | 0.7636 | 0.8691 | 0.6831 | 0.0541 |
| 25 | 0.7894 | 0.8610 | 0.7299 | 0.0401 |
| 30 | 0.7773 | 0.8408 | 0.6958 | 0.0424 |

**Finding:** 25 epochs provided the most consistent results (lowest std dev), but 15 epochs achieved the highest peak F1 score. The best model needed only 15 epochs.

### Workers Impact

| Workers | Avg F1 | Max F1 | Min F1 | Std Dev |
|---------|--------|--------|--------|---------|
| 2 | 0.7665 | 0.8521 | 0.6933 | 0.0469 |
| 4 | 0.7732 | 0.8964 | 0.6524 | 0.0593 |

**Finding:** 4 workers achieved the highest F1 score (0.8964), though with slightly more variance than 2 workers.

### Patch Size Impact

| Patch Size | Avg F1 | Max F1 | Min F1 | Std Dev |
|------------|--------|--------|--------|---------|
| 800 | 0.7671 | 0.8964 | 0.6524 | 0.0561 |
| 1000 | 0.7541 | 0.8691 | 0.6933 | 0.0460 |
| 1200 | 0.7811 | 0.8610 | 0.6957 | 0.0426 |

**Finding:** Patch size 800 achieved the highest F1 score (0.8964). While 1200 showed more consistency, 800 was optimal for peak performance.

---

## Key Insights

### Perfect Recall Configurations
**10 experiments achieved perfect recall (1.0)**, demonstrating complete detection of all ground truth trees:

- Exp 4 (F1=0.8964): batch=8, epochs=15, workers=4, patch=800
- Exp 35 (F1=0.8691): batch=16, epochs=20, workers=4, patch=1000
- Exp 66 (F1=0.8610): batch=32, epochs=25, workers=4, patch=1200
- Exp 24 (F1=0.8408): batch=8, epochs=30, workers=4, patch=1200
- Exp 37 (F1=0.8386): batch=16, epochs=25, workers=2, patch=800
- Exp 25 (F1=0.8021): batch=16, epochs=15, workers=2, patch=800
- Exp 53 (F1=0.7886): batch=32, epochs=15, workers=4, patch=1000
- Exp 33 (F1=0.7851): batch=16, epochs=20, workers=2, patch=1200
- Exp 63 (F1=0.8058): batch=32, epochs=25, workers=2, patch=1200
- Exp 40 (F1=0.8058): batch=16, epochs=25, workers=4, patch=800

### Optimal Configuration Pattern
The best-performing models share these characteristics:
- **Small batch sizes (8-16)** for better gradient updates
- **Smaller patch sizes (800-1000)** for finer spatial detail
- **4 workers** for optimal data loading parallelism
- **Moderate training duration (15-25 epochs)** - more epochs don't guarantee improvement

### Precision-Recall Tradeoff
- High precision (>0.80): Exp 4 (0.8123), Exp 62 (0.7810)
- Perfect recall (1.00): 10 experiments achieved no false negatives
- Best balance: Exp 4 with 0.8123 precision and 1.0 recall

### Training Efficiency
- Fastest training: 32.4s (Exp 1: batch=8, epochs=15)
- Slowest training: 85.9s (Exp 47: batch=16, epochs=30, workers=4)
- **Best model trained in just 37.23 seconds**

---

## Recommendations

### For Production Deployment
**Use Experiment #4 configuration:**
- Batch size: 8
- Epochs: 15
- Workers: 4
- Patch size: 800
- **Achieves 89.64% F1 with perfect recall**

### For Further Optimization
Consider exploring:
1. **Learning rate tuning** around the Exp 4 configuration
2. **Patch overlap strategies** with 800px patches
3. **Data augmentation** to improve precision while maintaining perfect recall
4. **Fine-grained search** near batch_size=8, epochs=15-20, patch_size=800

### When to Use Alternatives
- **For consistency**: Exp 62 (batch=32, epochs=25, workers=2, patch=1000) - high F1 (0.8521) with lower variance
- **For speed**: Exp 1 (batch=8, epochs=15, workers=2, patch=800) - trains in 32s with F1=0.7713
- **For large images**: Exp 66 (batch=32, epochs=25, workers=4, patch=1200) - handles larger patches with F1=0.8610

---

## Experiment Statistics

| Metric | Value |
|--------|-------|
| Total experiments | 72 |
| Completed successfully | 72 (100%) |
| Failed experiments | 0 (0%) |
| Total training time | ~69 minutes |
| Avg training time per experiment | 57.6 seconds |
| Avg evaluation time | 1.26 seconds |
| F1 score range | 0.6524 - 0.8964 |
| Mean F1 score | 0.7730 |
| Median F1 score | 0.7754 |
| F1 standard deviation | 0.0526 |

---

## Conclusion

The hyperparameter optimization successfully identified an optimal configuration (**Experiment #4**) achieving **89.64% F1 score with perfect recall**. This model strikes an excellent balance between detection accuracy and computational efficiency, training in under 40 seconds.

The results demonstrate that:
1. **Smaller batch sizes** (8) and **smaller patch sizes** (800) are optimal for this dataset
2. **Early stopping is viable** - the best model needed only 15 epochs
3. **4 workers** provide optimal data loading performance
4. **Perfect recall** is achievable while maintaining high precision (81.23%)

Model saved at: `full_optimization_results/best_model_exp_4.pt`

---

**Generated:** 2025-09-30
**Duration:** 72 experiments completed in 71 minutes
**Next Steps:** Deploy Experiment #4 model for production tree detection
